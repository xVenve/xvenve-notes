\chapter{Análisis de la complejidad de los Algoritmos}\label{ch:análisis-de-la-complejidad-de-los-algoritmos}
\adjustbox{width=\textwidth,center}{%
  \begin{tikzcd}
                                      &                                                             & \text{Complejidad Computacional} \arrow[d] &                                       \\
                                      &                                                             & \text{tipos} \arrow[rd] \arrow[ld]         &                                       \\
                                      & \text{Complejidad temporal} \arrow[d]                       &                                            & \text{Complejidad espacial} \arrow[d] \\
                                      & S(n) \arrow[d]                                              &                                            & T(n)                                  \\
                                      & \text{Número de pasos base} \arrow[ld] \arrow[d] \arrow[rd] &                                            &                                       \\
    \text{Máquina de Turing} \arrow[d] & \text{Ensamblador} \arrow[d]                                & \text{Lenguaje alto nivel*} \arrow[d]      &                                       \\
    \text{transición}                  & \text{instrucción}                                          & \text{operación elemental} \arrow[d]       &                                       \\
                                      &                                                             & \text{OE}                                  &
  \end{tikzcd}
}

* Estructuras de los algoritmos siempre se pueden considerar como estructuras de tipo Secuencial, Iterativo y Condicional, esto determina la operación elemental.

\section{Introducción}\label{sec:introduccion}
\textbf{Complejidad Computacional:} Recursos requeridos durante la ejecución de un algoritmo que da respuesta a un problema. Hay 2 tipos:
\begin{itemize}
  \item Tiempo: Número de pasos base de ejecución de un algoritmo para resolver un problema.
  \item Espacio: Memoria utilizada para resolver un problema. Registros, Memoria caché y Memoria RAM, etc.
\end{itemize}
Espacial y Temporal.

\textbf{Problema:} Una función entre el espacio de instancias y el espacio de respuestas, $X \rightarrow P(X)$.
\begin{itemize}
  \item Ejem. La ecuación de grado 2 tiene raíces cuadradas? $P(x^2+1=0)=\text{No}$
\end{itemize}

\textbf{Problema de decisión:} Tiene 2 respuestas: Si o No.
\pagebreak

\textbf{Instancia de un problema:} Es la especificación exacta de los datos de un problema para un caso particular.
\begin{itemize}
  \item \textbf{Tamaño de instancia:} Número de datos de la instancia.
  \item Ejem. Independiente, $x^2$ y $x$ son los 3 parámetros de una instancia de un problema de ecuación de segundo grado.
\end{itemize}

\textbf{Algoritmo:} Conjunto de instrucciones que garantiza encontrar una solución correcta para cualquier instancia en un número finito de pasos.

La complejidad puede hacer referencia a un algoritmo o un problema.
\begin{itemize}
  \item \textbf{Algoritmo:} Resolver las instancias de un problema. \\ Medida de número de pasos base requeridos para la PEOR instancia de tamaño n. \\ Relación entre n y el número de pasos.
  \item \textbf{Problema:} Es el objeto de estudio de la Teoría de la Complejidad Computacional.
\end{itemize}

\section{Paso Base}\label{sec:paso-base}
Se pueden modelar los algoritmos mediante:
\begin{enumerate}
    \item Construcción matemática denominada Máquina de Turing $\rightarrow$ Transición / Tabla de transiciones.
  \item Código Ensamblador $\rightarrow$ Instrucción \ Conjunto estructurado de instrucciones.
  \item Lenguaje de alto nivel $\rightarrow$ Operación Elemental / Conjunto estructurado de instrucciones.
    \begin{itemize}
      \item Suma, Restar División, Multiplicación, etc.
    \end{itemize}
\end{enumerate}

\section{Complejidad de los Algoritmos}\label{sec:complejidad-de-los-algoritmos}
Se expresa como: $T(n)$, número de pasos peor caso con entrada n.

La complejidad computacional puede referirse también a los recursos de espacio necesarios según el tamaño de la entrada. Así tenemos que:
\begin{itemize}
  \item Tiempo de ejecución (complejidad temporal) $T(n)$
  \item Espacio necesario para los datos (complejidad espacial) $S(n)$
\end{itemize}

Todo algoritmo puede ser implementado con 3 estructuras: Secuencial, Condicional e Iterativa.

\textbf{Operación Elemental (OE)}
\begin{itemize}
  \item Comparación
  \item Asignación variable
  \item Acceso a estructura básica
  \item Llamada función
  \item Retorno función
\end{itemize}

\textbf{Estructura Secuencial:} Cada I es una instancia o bloque, se calcula:
\begin{itemize}
  \item El de todos: $T(I_1; I_2; \dots; I_n) = T(I_1) + T(I_2) + \dots + T(I_n)$
  \item General: $T= \max \{T(I_1), T(I_2), \dots, T(I_n)\}$ El T es el más alto, el del peor caso/bloque.
  \item Para los programas es la suma de todas las estructuras: $T = T_1 + T_2 + \dots + T_k = \max \{ T_1, T_2, \dots, T_k \}$
\end{itemize}

\textbf{Estructura Condicional:}
\begin{itemize}
  \item if (Condición) then BloqueThen else BloqueElse
  \item $T = T(Condicion) + \max \{T(BloqueThen), T(BloqueElse)\}$
\end{itemize}

\textbf{Estructura Iterativa:}
\begin{itemize}
  \item while(C) S - $T = C + n_{iter} (C+S)$
  \item for(A C B) S - $T = A + C + n_{iter} (C+B+S)$
  \begin{itemize}
    \item A: Inicialización
    \item C: Condición
    \item B: Incremento
    \item S: Bloque
  \end{itemize}
\end{itemize}
\pagebreak

\textbf{Llamada a Procedimiento, Función o Método:}
\begin{itemize}
  \item $funcion(p_1, p_2, \dots, p_k) f$
  \item 1 (por la llamada) + tiempo de evaluación parámetros + tiempo que tarde en ejecutarse el ''cuerpo'' de la función.
  \item $T = 1 + T(p_1) + T(p_2) + \dots + T(p_n) + T(f)$
\end{itemize}

No se contabiliza la copia de los argumentos a la pila de ejecución salvo estructuras complejas (vector, registros) que se pasen por valor.

El paso por referencia y paso de punteros, no se contabiliza.

\subsection{Ejemplo}\label{subsec:ejemplo}
\begin{lstlisting}[language=Java,label={lst:lstlisting}]
for (int i = 0, acum = 0; i < n; i++)
  acum += i;
\end{lstlisting}

$T(n) = A + C + n ( C + B + S ) = 2 + 1 + n ( 1 + 2 + 2 ) = 3 + 5 n$

\begin{lstlisting}[language=Java,label={lst:lstlisting2}]
if (x < 5)
  for (int i = 0; i < n; i++)
    acum += i;
else
  acum = 3;
\end{lstlisting}

$T(n) = 1 + \max \{ 1 + 1 + n(1+2+2) ; 1 \} = 1 + \max \{ 2 + 5 n ; 1 \} = 3 + 5n$

\begin{lstlisting}[language=Java,label={lst:lstlisting3}]
for (int i = 0, acum = 0; i < n; i++)
  for (int j = 0; j < i/2; j++)
    acum += i;
\end{lstlisting}

$T(n) = 2 + 1 + n ( 1 + 2 + [ 1 + 2 + \frac{n}{2} ( 2 + 2 + 2) ]) = 3 + n (3 + 3 + 3 n) = 3 + 6 n + 3 n^2$

\begin{lstlisting}[language=Java,label={lst:lstlisting4}]
x = 5;
while (x < N)
  x = 3 * x;
\end{lstlisting}

$T(n) = 1 + 1 + p ( 1 + 2 ) = 2 + 3 p$

$5 \cdot 3^p < n; p \log 3 < \log \frac{n}{7}; p = \frac{\log \frac{n}{5}}{\log 3}$
\pagebreak

\section{Algoritmos Recursivos}\label{sec:algoritmos-recursivos}
\adjustbox{width=\textwidth,center}{%
\begin{tikzcd}
  & \text{Algoritmo recursivo } \text{(Tiene coste } T(n) \text{)} \arrow[d] &                           \\
  & \text{Ecuaciones de recurrencia } E(n) \arrow[ld] \arrow[rd]     &                           \\
\text{Ecuacion caracteristica (Polinomica } P(n)=0 \text{)} \arrow[d] &                                                                  & \text{Cambio de variable} \\
{\text{Obtenemos } x_1, x_2, x_3, ..., x_n} \arrow[d]                 &                                                                  &                           \\
\text{Obtenemos de } T(n) \text{ no recursiva}                        &                                                                  &                          
\end{tikzcd}
}

El coste ($T(n)$) de un algoritmo recursivo será también recursivo.

$T(n)= E(n)$ En la expresión 
\begin{itemize}
  \item $E$ aparece la propia función $T$.
  \item El objetivo es encontrar $T(n)= f(n)$, sin funciones $T$.
\end{itemize}

Resolver la ecuación de recurrencia. Dos métodos:
\begin{enumerate}
  \item Despliegue de la recurrencia
  \item Resolución general de recurrencias
  \begin{enumerate}
    \item Recurrencia homogénea (ecuación característica)
    \item Recurrencia útiles (fórmulas maestras)
    \item Recurrencias no homogéneas y no lineales (no se tratan en este curso)
  \end{enumerate}
  
\end{enumerate}
  
\subsection{Despliegue de Recurrencias}
Aplicar varias veces la fórmula recurrente hasta obtener una fórmula general que relaciona la función para el tamaño original con otros tamaños menores. A partir de esta fórmula se obtiene otra que la relaciona con el caso base.

Pasos:
\begin{enumerate}
  \item Ecuación ($T(n)=...$)
  \item Despliegue, ($T(n-1), T(n-2), etc$)
  \item Aplicaciones del caso base para $n=0, ..., k\;\; T(0)=a, T(1)=b, ...$
  \item Resolución
\end{enumerate}

No debe aplicarse este método si aparecen varios términos recurrentes, como es el caso de la serie de Fibonacci.

Se parte de un caso base y se va sustituyendo dentro de la ecuación el propio término, de manera que cuando vamos sustituyendo y resolviendo la ecuación hasta llegar a un caso en que podamos aplicar el caso base. En ese punto podremos sustituir nuestro caso base en la ecuación y quitarnos completamente la recurrencia, t().

Lo que buscamos es que al desdoblar los términos constantemente encontremos una regularidad. En ese momento hacemos que una variable valga un valor que podamos utilizar para usar el caso base, normalmente t(1).

Ejem. Factorial
\begin{itemize}
  \item $t(1) = 1 \textit{ y } t(n) = t(n-1)+1$
  \item $t(n) = t(n-2)+1+1=t(n-2)+2=t(n-3)+1+2=t(n-3)+3=t(n-k)+k$
  \item Sea $k=n-1$ $t(n)=t(n-(n-1))+n-1=t(1)+n-1=n$
\end{itemize}

\subsection{Resolución General de la Ecuación de Recurrencia}
$c_nt(n)+c_{n-1}t(n-1)+...+c_{n-k}t(n-k)=b$, expresión en n.

Si $b=0$, se dice que es Ecuación de Recurrencia Lineal (de orden k) HOMOGÉNEA

Los términos de las ecuaciones de recurrencia están afectados por coeficientes. Debemos encontrar una expresión no recursiva de $t$.

No nos ocuparemos de ecuaciones en las que el coeficiente de $t(n-k)$, i.e. $c_{n-k}$ es variable (depende de n), trataremos solo con: ECUACIONES DE RECURRENCIA LINEALES CON COEFICIENTES CONSTANTES.

\subsubsection{Ecuación de Recurrencia Lineal Homogenea}
Método de la Ecuación Característica.
\begin{itemize}
  \item A una ecuación de recurrencia de orden k, mediante un cambio de variable, se le asocia una ecuación polinómica de grado k (la ecuación característica).
  \item Las soluciones de la ecuación polinómica proporcionan las soluciones de la ecuación de recurrencia.
  \item Solución general de la ecuación de recurrencia:
  \begin{itemize}
    \item Es combinación lineal de las soluciones básicas.
    \item Las constantes de la combinación lineal se calculan imponiendo los casos base.
    \item Con ello obtenemos la solución (no recursiva) de la recurrencia $T(n)$
  \end{itemize}

\end{itemize}

Cambio de variable: $T(n)= x^n$, se obtiene la ecuación característica $c_n x^n+ c_{n-1}x^{n-1}+ ... + c_{n-k}x^{n-k} = 0$ siendo $r_1, r_2, ..., r_k$ las raíces reales o complejas.

A una ecuación de recurrencia de orden k, mediante un cambio de variable, se le asocia una ecuación polinómica de grado k (la ecuación característica).

Lo que hacemos tras la sustitución es obtener las raíces, que pueden ser distintas $x=2$ y $x=3$ o iguales $x=1$, $(x-1) \cdot (x-1)$.
\begin{itemize}
  \item Para las raíces distintas se aplica:
  $$T(n) = \sum _1 ^k b_i r_i^n, \; b_i, r_i \in \mathbb{R}$$
  $r_1$ y $r_2$ son raíces diferentes, $T(n)=b_1r_1^n+b_2r_2^n$ es solución.
  \item Una raíz con multiplicidad mayor que 1:
  $$T(n) = \sum _{i=1} ^m b_i n^{i-1}r_1^n+\sum _{i=m+1} ^k b_i r_{i-m+1}^n, \; b_i, r_i \in \mathbb{R}$$
  $r_1$ son las raíces de la ecuación característica y $m$ la multiplicidad cuando es mayor que 1.\\
  $r$  raíz doble, $T(n)=b_1nr^n+b_2r^n$ es solución.
\end{itemize}

Se cambia el $t(n)$, $t(n-1)$, \ldots por variables $x$ que dependan del $n$.

El grado de la variable siempre es un grado menos que el número de términos recurrentes y cada uno de valor menor reduce el grado.

Ejem. $T(n)=T(n-1)+T(n-1) \rightarrow x^2= x + 1$
\begin{itemize}
  \item Sus raíces son $r = \frac{1\pm \sqrt{5}}{2}$, estos términos se sustituirían en las ecuaciones de arriba quedando:
  \item Sabiendo $T(0) = 0$ y $T(1)=1$
  \begin{itemize}
    \item $T(n)= b_1(\frac{1+ \sqrt{5}}{2})^n + b_2(\frac{1- \sqrt{5}}{2})^n$
    \item $T(0)= b_1(\frac{1+ \sqrt{5}}{2})^0 + b_2(\frac{1- \sqrt{5}}{2})^0 = b_1 + b_2$
    \item $T(1)= b_1(\frac{1+ \sqrt{5}}{2})^1 + b_2(\frac{1- \sqrt{5}}{2})^1 = 1$
  \end{itemize}
  \item Despejamos con ambas ecuaciones: $b_1 = -b_2 = \frac{1}{5}$.
\end{itemize}

\subsection{Fractales}
Fractal, división de figuras geométricas.

Entrada: Figura geométrica y como parámetro un número natural.

Se trata de obtener la figura geométrica resultante de dividir la anterior en 4 partes, hasta el número dado por la resolución.

Ecuaciones temporales, expresadas en función de la resolución.
\begin{itemize}
  \item Ejem. Triángulo $T(0)=c$ y $T(n)=3\cdot T(n-1)+d$
  \item Sol. $T(n)= c3^n+ \frac{3^n-1}{2}d=O(3^n)$
\end{itemize}

\section{Cotas asintóticas}
Peores instancias de tamaño de entrada n: $O(g(n))$ Cota superior.

Mejores instancias de tamaño de entrada n: $\Omega(g(n))$ Cota inferior.

Instancias medias: $\Theta(g(n))$ Cota ajustada.

\begin{table}[H]
  \begin{tabular}{|c|c|}
  \hline
  \rowcolor[HTML]{BFBFBF} 
  Notación                            & Orden - nombre                               \\ \hline
  $O(1)$                                & Constante                                    \\ \hline
  $O(\log n)$                            & Logarítmico                                  \\ \hline
  $O([\log n]^c)$                        & Polilogarítmico                              \\ \hline
  $O(n)$         & Lineal                \\ \hline
  $O(n \cdot \log n)$ & Lineal logarítmico    \\ \hline
  $O(n^2)$       & Cuadrático            \\ \hline
  $O(n^c)$       & Polinómico            \\ \hline
  $O(c^n)$       & Exponencial           \\ \hline
  $O(n!)$        & Factorial             \\ \hline
  $O(n^n)$       & Potencial-exponencial \\ \hline
  \end{tabular}
  \caption{Nombres de los órdenes de complejidad}
\end{table}

\subsection{Cota Superior Asintótica}
Es una función, g(n), que sirve de cota superior de otra función, f(n), cuando $n\rightarrow \infty$. f(n) no puede pasar de g(n).

Se utilizan la notación de Landau Big O $O(g(n))$ para referirse al Conjunto de las funciones $f(n)$ acotadas superiormente por la función $g(n)$.
\begin{itemize}
  \item $f(n)$ $\in$ $O(g(n))$
  \item $f(n)$ es $O(g(n))$
  \item $f(n)$ es $o$ de $g(n)$
\end{itemize}

$$O(g(n))=\left\{\begin{matrix}
  f(n)\;/\;\exists c, n_0 \textit{ constantes positivas tales que}  \\
  \forall n \geq n_0, f(n) \leq c \cdot g(n)
  \end{matrix}\right\}$$

$$f(n)=O(g(n)) \Leftrightarrow \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}= k \in \mathbb{R}, k \geq 0$$

Se hacen intervalos de signos de la función $h(n) = f(n)-g(n)$ para ver en que momentos nuestra $g(n)$ sobrepasa $f(n)$ y no vuelve a superarla.

Hay que fijarse a partir del último corte que no ocurra que $f(n) > g(n)$. Puede ocurrir a la izquierda, pero nunca más a la derecha.

Algunas propiedades:
\begin{itemize}
  \item $O(a \cdot f(n))= O(f(n))$ con $a \in \mathbb{R}$
  \item $O(\log_a n) = O(\log_b n)$ con $a,b > 1$
  \item Si $f \in O(g)$ y $g \in O(h)$ entonces $f \in O(h)$
  \item Regla de la suma: $O(f+g)=O(\max(f, g))$
  \item Regla del producto: Si $g_1 \in O(f_1)$ y $g_2 \in O(f_2)$ entonces $g_1 \cdot g_2 \in O(f_1 \cdot f_2)$
\end{itemize}

\subsection{o (o pequeña / ómicron minúscula)}
$f(n)=o(g(n))$ $\exists c, n_0\;/\; f(n)<c\cdot g(n), \; \forall n \geq n_0$ con $n_0$ positivo. $\lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}=0$

Nunca la va a poder alcanzar.

\subsection{Cota Inferior Asintótica}
Función que sirve de cota inferior de otra función cuando el argumento tiende a infinito.

Se usa para calcular la complejidad del mejor caso para los algoritmos.

$$\Omega(g(n))=\left\{\begin{matrix}
  f(n)\;/\;\exists c, n_0 \textit{ constantes positivas tales que}  \\
  \forall n \geq n_0, f(n) \geq c \cdot g(n)
  \end{matrix}\right\}$$

$$f(n)=\Omega(g(n)) \Leftrightarrow \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}= k > 0 (\textit{puede ser }\infty)$$

\subsection{Cota Ajustada Asintótica}
$$\Theta(g(n))=\left\{\begin{matrix}
  f(n)\;/\;\exists C_1 \;y\; C_2 >0, \exists n_0>0, \textit{ tal que } \forall n \geq n_0  \\
  C_1 \cdot g(n) \leq f(n) \leq C_2 \cdot g(n)
  \end{matrix}\right\}$$

$$f(n)=\Theta(g(n)) \Leftrightarrow \lim_{n \rightarrow \infty} \frac{f(n)}{g(n)}= k \in \mathbb{R}, k > 0$$

$$f = \Theta(g) \Leftrightarrow f = O(g) \wedge f = \Omega(g) $$

\section{Trazabilidad}\label{sec:tratabilidad}


\section{Análisis Asintótico}\label{sec:análisis-asintótico}